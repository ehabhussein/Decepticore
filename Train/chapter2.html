<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Sentence Transformers</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Chapter 2: Sentence Transformers</h1>
    </header>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="chapter1.html">1. Understanding LLM Finetuning</a></li>
            <li><a href="chapter2.html">2. Sentence Transformers</a></li>
            <li><a href="chapter3.html">3. Other Text Tasks</a></li>
            <li><a href="chapter4.html">4. Image Tasks</a></li>
            <li><a href="chapter5.html">5. Tabular Tasks</a></li>
            <li><a href="chapter6.html">6. Training Parameters Explained</a></li>
            <li><a href="chapter7.html">7. Additional Parameters</a></li>
        </ul>
    </nav>
    <main>
        <section>
            <h2>2.1 Introduction to Sentence Transformers</h2>
            <p>Sentence Transformers are neural network models designed to map sentences and paragraphs to a dense vector space, enabling semantic similarity comparisons and various NLP tasks.</p>
        </section>
        <section>
            <h2>2.2 Key Concepts</h2>
            <ul>
                <li><strong>Embedding:</strong> Dense vector representations of sentences or paragraphs.</li>
                <li><strong>Semantic Similarity:</strong> Measuring how close two pieces of text are in meaning.</li>
                <li><strong>Transfer Learning:</strong> Leveraging pre-trained models for specific downstream tasks.</li>
            </ul>
        </section>
        <section>
            <h2>2.3 Training Techniques</h2>
            <h3>2.3.1 ST Pair</h3>
            <p>Training with sentence pairs for tasks like similarity measurement.</p>
            
            <h3>2.3.2 ST Pair Classification</h3>
            <p>Classification task using sentence pairs, useful for tasks like natural language inference.</p>
            
            <h3>2.3.3 ST Pair Scoring</h3>
            <p>Scoring sentence pairs based on certain criteria, applicable in information retrieval tasks.</p>
            
            <h3>2.3.4 ST Triplet</h3>
            <p>Training using triplets of sentences (anchor, positive, negative) for better contextual embeddings.</p>
            
            <h3>2.3.5 ST Question Answering</h3>
            <p>Training for question-answering tasks, enhancing the model's ability to understand and answer questions based on context.</p>
        </section>
        <section>
            <h2>2.4 Applications</h2>
            <ul>
                <li>Semantic Search</li>
                <li>Clustering and Document Organization</li>
                <li>Duplicate Detection</li>
                <li>Information Retrieval</li>
                <li>Paraphrase Identification</li>
            </ul>
        </section>
        <section>
            <h2>2.5 Best Practices</h2>
            <ul>
                <li>Choose appropriate pre-trained models</li>
                <li>Fine-tune on domain-specific data when possible</li>
                <li>Use task-appropriate loss functions</li>
                <li>Evaluate using relevant metrics for your specific application</li>
                <li>Consider model size vs. performance trade-offs</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 LLM Finetuning Guide. All rights reserved.</p>
        <nav>
            <a href="chapter1.html">Previous: Understanding LLM Finetuning</a> | 
            <a href="chapter3.html">Next: Other Text Tasks</a>
        </nav>
    </footer>
</body>
</html>
